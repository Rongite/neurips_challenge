{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Environment check and basic imports\n",
    "import sys, torch, platform\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Arch   :\", platform.machine())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# üéØ HYPERPARAMETER TUNING VARIABLES\n",
    "# =====================================\n",
    "# Modify these variables to tune model performance\n",
    "# These will override the values in polymer-GRIT-RRWP.yaml\n",
    "\n",
    "print(\"üéõÔ∏è Setting up hyperparameter tuning variables...\")\n",
    "\n",
    "# ==== HIGH PRIORITY PARAMETERS (Major Impact on Performance) ====\n",
    "\n",
    "# Learning Rate - Controls training speed and convergence\n",
    "# Recommended: [1e-4, 5e-4, 1e-3, 2e-3]\n",
    "BASE_LR = 1e-3\n",
    "\n",
    "# Model Depth - Number of Transformer layers  \n",
    "# Recommended: [8, 10, 12, 14] (more layers = more capacity but slower)\n",
    "GT_LAYERS = 10\n",
    "\n",
    "# Hidden Dimension - Model width/capacity\n",
    "# Recommended: [64, 128, 192] (higher = more capacity but more memory)\n",
    "GT_DIM_HIDDEN = 64\n",
    "\n",
    "# Batch Size - Training batch size\n",
    "# Recommended: [16, 32, 64] (higher = more stable but more memory)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ==== MEDIUM PRIORITY PARAMETERS (Moderate Impact) ====\n",
    "\n",
    "# Dropout Rate - Regularization strength\n",
    "# Recommended: [0.0, 0.1, 0.2] (higher = more regularization)\n",
    "GT_DROPOUT = 0.0\n",
    "\n",
    "# Attention Heads - Multi-head attention\n",
    "# Recommended: [4, 6, 8, 12] (should divide dim_hidden evenly)\n",
    "GT_N_HEADS = 8\n",
    "\n",
    "# Weight Decay - L2 regularization\n",
    "# Recommended: [1e-6, 1e-5, 1e-4] (higher = more regularization)  \n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# Training Epochs - Total training steps\n",
    "# Recommended: [150, 200, 300] (more = longer training)\n",
    "MAX_EPOCH = 200\n",
    "\n",
    "# Attention Dropout - Regularization for attention mechanism\n",
    "# Recommended: [0.0, 0.1, 0.2, 0.3]\n",
    "ATTN_DROPOUT = 0.2\n",
    "\n",
    "# ==== PARAMETER SOURCE CONFIRMATION ====\n",
    "print(\"=\" * 80)\n",
    "print(\"üîî PARAMETER SOURCE CONFIRMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ USING NOTEBOOK HYPERPARAMETER VARIABLES (NOT polymer-GRIT-RRWP.yaml)\")\n",
    "print(\"üìã The following parameters will OVERRIDE the YAML file:\")\n",
    "\n",
    "print(\"\\nüìä Current Hyperparameter Settings (FROM NOTEBOOK VARIABLES):\")\n",
    "print(f\"   üéØ Learning Rate (BASE_LR): {BASE_LR} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Model Layers (GT_LAYERS): {GT_LAYERS} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Hidden Dimension (GT_DIM_HIDDEN): {GT_DIM_HIDDEN} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Batch Size (BATCH_SIZE): {BATCH_SIZE} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Dropout (GT_DROPOUT): {GT_DROPOUT} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Attention Heads (GT_N_HEADS): {GT_N_HEADS} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Weight Decay (WEIGHT_DECAY): {WEIGHT_DECAY} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Max Epochs (MAX_EPOCH): {MAX_EPOCH} ‚Üê FROM NOTEBOOK\")\n",
    "print(f\"   üéØ Attention Dropout (ATTN_DROPOUT): {ATTN_DROPOUT} ‚Üê FROM NOTEBOOK\")\n",
    "\n",
    "# ==== VALIDATION CHECKS ====\n",
    "print(\"\\nüîç Parameter Validation:\")\n",
    "# Check if attention heads divide hidden dimension evenly\n",
    "if GT_DIM_HIDDEN % GT_N_HEADS != 0:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: GT_DIM_HIDDEN ({GT_DIM_HIDDEN}) should be divisible by GT_N_HEADS ({GT_N_HEADS})\")\n",
    "    print(f\"   Recommended GT_N_HEADS for dim_hidden={GT_DIM_HIDDEN}: {[i for i in [4,6,8,12,16] if GT_DIM_HIDDEN % i == 0]}\")\n",
    "else:\n",
    "    print(f\"‚úÖ GT_DIM_HIDDEN ({GT_DIM_HIDDEN}) is divisible by GT_N_HEADS ({GT_N_HEADS})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Hyperparameter variables initialized successfully!\")\n",
    "print(\"üéØ These values will be used instead of polymer-GRIT-RRWP.yaml\")\n",
    "print(\"üí° To tune performance, modify the variables above and re-run this cell\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T00:44:15.181605Z",
     "iopub.status.busy": "2025-08-07T00:44:15.181268Z",
     "iopub.status.idle": "2025-08-07T00:45:23.359325Z",
     "shell.execute_reply": "2025-08-07T00:45:23.357989Z",
     "shell.execute_reply.started": "2025-08-07T00:44:15.181582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle Environment Setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kaggle paths\n",
    "TRAIN_CSV = Path('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\n",
    "TEST_CSV = Path('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')  \n",
    "SUPPLEMENT_DIR = Path('/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement')\n",
    "GRIT_SOURCE = Path('/kaggle/input/grit/pytorch/default/1/neurips_challenge/GRIT')\n",
    "PIPELINE_SOURCE = Path('/kaggle/input/grit/pytorch/default/1/neurips_challenge/full_pipeline.py')\n",
    "CONFIG_SOURCE = Path('/kaggle/input/grit/pytorch/default/1/neurips_challenge/configs')\n",
    "KAGGLE_WORKING = Path('/kaggle/working')\n",
    "\n",
    "print(\"üîç Kaggle Environment Setup\")\n",
    "print(f\"üìñ Pipeline Source: {PIPELINE_SOURCE}\")\n",
    "print(f\"üìñ Config Source: {CONFIG_SOURCE}\")\n",
    "print(f\"‚úèÔ∏è  Working Directory: {KAGGLE_WORKING}\")\n",
    "\n",
    "# Install offline wheels\n",
    "try:\n",
    "    exec(open('/kaggle/input/grit-wheels-supplement/neurips-offline-wheels-truly-offline/install_offline.py').read())\n",
    "    exec(open('/kaggle/input/grit-wheels/install_offline.py').read())\n",
    "    print(\"‚úÖ Offline wheels installed\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Offline wheels installation failed (expected in local testing)\")\n",
    "\n",
    "# Create working directories\n",
    "working_dirs = ['graphs', 'results', 'cfg_runs', 'checkpoints', 'logs']\n",
    "for subdir in working_dirs:\n",
    "    (KAGGLE_WORKING / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(KAGGLE_WORKING))\n",
    "print(f\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-07T00:45:54.981520Z",
     "iopub.status.busy": "2025-08-07T00:45:54.981176Z",
     "iopub.status.idle": "2025-08-07T00:45:55.011240Z",
     "shell.execute_reply": "2025-08-07T00:45:55.009683Z",
     "shell.execute_reply.started": "2025-08-07T00:45:54.981496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Dynamic Config with Notebook Hyperparameters\n",
    "import yaml\n",
    "\n",
    "print(\"üéõÔ∏è Creating config with notebook hyperparameters...\")\n",
    "\n",
    "# Read original config from Kaggle input\n",
    "original_config_path = CONFIG_SOURCE / 'polymer-GRIT-RRWP.yaml'\n",
    "with open(original_config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"üìñ Read base config from: {original_config_path}\")\n",
    "\n",
    "# Override with notebook hyperparameters  \n",
    "config['train']['batch_size'] = BATCH_SIZE\n",
    "config['gt']['layers'] = GT_LAYERS\n",
    "config['gt']['n_heads'] = GT_N_HEADS\n",
    "config['gt']['dim_hidden'] = GT_DIM_HIDDEN\n",
    "config['gt']['dropout'] = GT_DROPOUT\n",
    "config['gt']['attn_dropout'] = ATTN_DROPOUT\n",
    "config['optim']['base_lr'] = BASE_LR\n",
    "config['optim']['weight_decay'] = WEIGHT_DECAY  \n",
    "config['optim']['max_epoch'] = MAX_EPOCH\n",
    "\n",
    "# Adaptive settings\n",
    "config['optim']['num_warmup_epochs'] = max(10, MAX_EPOCH // 4)\n",
    "config['optim']['min_lr'] = BASE_LR / 100\n",
    "config['gnn']['dim_inner'] = GT_DIM_HIDDEN\n",
    "config['gnn']['dropout'] = GT_DROPOUT\n",
    "\n",
    "# Disable tensorboard for Kaggle compatibility\n",
    "config['tensorboard_each_run'] = False\n",
    "\n",
    "# Save to working directory\n",
    "dynamic_config_path = KAGGLE_WORKING / 'polymer-GRIT-RRWP.yaml'\n",
    "with open(dynamic_config_path, 'w') as f:\n",
    "    yaml.safe_dump(config, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "print(f\"‚úÖ Dynamic config saved: {dynamic_config_path}\")\n",
    "\n",
    "print(f\"\\nüìä Using Notebook Hyperparameters:\")\n",
    "print(f\"  üéØ Learning Rate: {BASE_LR}\")\n",
    "print(f\"  üéØ GT Layers: {GT_LAYERS}\")\n",
    "print(f\"  üéØ Hidden Dimension: {GT_DIM_HIDDEN}\")  \n",
    "print(f\"  üéØ Attention Heads: {GT_N_HEADS}\")\n",
    "print(f\"  üéØ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  üéØ Dropout: {GT_DROPOUT}\")\n",
    "print(f\"  üéØ Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  üéØ Max Epochs: {MAX_EPOCH}\")\n",
    "print(f\"  üìä TensorBoard: Disabled (Kaggle compatibility)\")\n",
    "\n",
    "print(f\"\\nüîî CONFIG CONFIRMED: Using notebook variables instead of original YAML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T00:46:10.143250Z",
     "iopub.status.busy": "2025-08-07T00:46:10.142809Z",
     "iopub.status.idle": "2025-08-07T00:46:13.524469Z",
     "shell.execute_reply": "2025-08-07T00:46:13.523099Z",
     "shell.execute_reply.started": "2025-08-07T00:46:10.143219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Execute Pipeline with Notebook Hyperparameters\n",
    "import importlib.util\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Executing full_pipeline.py with notebook hyperparameters...\")\n",
    "\n",
    "# Read and adapt pipeline for Kaggle paths\n",
    "with open(PIPELINE_SOURCE, 'r') as f:\n",
    "    pipeline_code = f.read()\n",
    "\n",
    "# Apply comprehensive Kaggle path fixes\n",
    "kaggle_fixes = {\n",
    "    # Basic paths\n",
    "    'GRIT_DIR = Path(__file__).resolve().parent / \"GRIT\"': 'GRIT_DIR = Path(\"/kaggle/working/GRIT\")',\n",
    "    'ROOT        = Path(__file__).resolve().parent': 'ROOT = Path(\"/kaggle/working\")',\n",
    "    'DATA_ROOT   = ROOT / \"data\"': 'DATA_ROOT = Path(\"/kaggle/input/neurips-open-polymer-prediction-2025\")',\n",
    "    'SUPP_DIR    = DATA_ROOT / \"train_supplement\"': 'SUPP_DIR = Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement\")',\n",
    "    'GRAPH_DIR   = SUPP_DIR / \"graphs\"': 'GRAPH_DIR = Path(\"/kaggle/working/graphs\")',\n",
    "    'RESULTS_DIR = ROOT / \"results\"': 'RESULTS_DIR = Path(\"/kaggle/working/results\")',\n",
    "    \"sub_out.to_csv(ROOT/'submission.csv', index=False)\": 'sub_out.to_csv(\"/kaggle/working/submission.csv\", index=False)',\n",
    "    'dataset = PolymerDS_class(root=DATA_ROOT, target_idx=gym_cfg.dataset.target_idx)': 'dataset = PolymerDS_class(root=Path(\"/kaggle/working\"), target_idx=gym_cfg.dataset.target_idx)',\n",
    "    \n",
    "    # Fix the train_supplement/graphs path issue\n",
    "    'Path(root) / \"train_supplement\" / \"graphs\" / \"train_graphs.pt\"': 'Path(\"/kaggle/working/graphs/train_graphs.pt\")',\n",
    "    'Path(root) / \"train_supplement\" / \"graphs\" / \"test_graphs.pt\"': 'Path(\"/kaggle/working/graphs/test_graphs.pt\")',\n",
    "    \n",
    "    # Additional graph file references\n",
    "    'torch.save(graphs, GRAPH_DIR / \"train_graphs.pt\")': 'torch.save(graphs, Path(\"/kaggle/working/graphs/train_graphs.pt\"))',\n",
    "    'torch.save(t_graphs, GRAPH_DIR / \"test_graphs.pt\")': 'torch.save(t_graphs, Path(\"/kaggle/working/graphs/test_graphs.pt\"))',\n",
    "    'torch.load(GRAPH_DIR / \"train_graphs.pt\"': 'torch.load(Path(\"/kaggle/working/graphs/train_graphs.pt\")',\n",
    "    'torch.load(GRAPH_DIR / \"test_graphs.pt\"': 'torch.load(Path(\"/kaggle/working/graphs/test_graphs.pt\")',\n",
    "    '(GRAPH_DIR / \"train_graphs.pt\").exists()': 'Path(\"/kaggle/working/graphs/train_graphs.pt\").exists()',\n",
    "    '(GRAPH_DIR / \"test_graphs.pt\").exists()': 'Path(\"/kaggle/working/graphs/test_graphs.pt\").exists()',\n",
    "    \n",
    "    # Dataset file references\n",
    "    'DATA_ROOT / \"train.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\")',\n",
    "    'DATA_ROOT / \"test.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\")',\n",
    "    'SUPP_DIR / \"dataset1.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset1.csv\")',\n",
    "    'SUPP_DIR / \"dataset2.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset2.csv\")',\n",
    "    'SUPP_DIR / \"dataset3.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset3.csv\")',\n",
    "    'SUPP_DIR / \"dataset4.csv\"': 'Path(\"/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv\")',\n",
    "    \n",
    "    # Other results paths\n",
    "    'report_path = save_dir / \"stage1_evaluation_report.csv\"': 'report_path = Path(\"/kaggle/working/stage1_evaluation_report.csv\")',\n",
    "    'CONFIG_SAVE = RESULTS_DIR / \"cfg_runs\"': 'CONFIG_SAVE = Path(\"/kaggle/working/cfg_runs\")',\n",
    "}\n",
    "\n",
    "for old, new in kaggle_fixes.items():\n",
    "    pipeline_code = pipeline_code.replace(old, new)\n",
    "\n",
    "# Save adapted pipeline\n",
    "kaggle_pipeline = KAGGLE_WORKING / 'full_pipeline_kaggle.py' \n",
    "with open(kaggle_pipeline, 'w') as f:\n",
    "    f.write(pipeline_code)\n",
    "\n",
    "print(f\"‚úÖ Pipeline adapted for Kaggle: {kaggle_pipeline}\")\n",
    "\n",
    "# Copy and patch GRIT \n",
    "try:\n",
    "    import shutil\n",
    "    if GRIT_SOURCE.exists():\n",
    "        writable_grit = KAGGLE_WORKING / \"GRIT\"\n",
    "        if writable_grit.exists():\n",
    "            shutil.rmtree(writable_grit)\n",
    "        shutil.copytree(GRIT_SOURCE, writable_grit)\n",
    "        print(f\"‚úÖ GRIT copied to: {writable_grit}\")\n",
    "        \n",
    "        # Fix OGB smiles2graph import issue\n",
    "        print(\"üîß Patching OGB smiles2graph imports...\")\n",
    "        ogb_implementation = '''# ===== OGB SMILES2GRAPH IMPLEMENTATION =====\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "allowable_features = {\n",
    "    'possible_atomic_num_list': list(range(1, 119)) + ['misc'],\n",
    "    'possible_chirality_list': ['CHI_UNSPECIFIED', 'CHI_TETRAHEDRAL_CW', 'CHI_TETRAHEDRAL_CCW', 'CHI_OTHER', 'misc'],\n",
    "    'possible_degree_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'possible_formal_charge_list': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'possible_numH_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 'misc'],\n",
    "    'possible_number_radical_e_list': [0, 1, 2, 3, 4, 'misc'],\n",
    "    'possible_hybridization_list': ['SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'],\n",
    "    'possible_is_aromatic_list': [False, True],\n",
    "    'possible_is_in_ring_list': [False, True],\n",
    "    'possible_bond_type_list': ['SINGLE', 'DOUBLE', 'TRIPLE', 'AROMATIC', 'misc'],\n",
    "    'possible_bond_stereo_list': ['STEREONONE', 'STEREOZ', 'STEREOE', 'STEREOCIS', 'STEREOTRANS', 'STEREOANY'],\n",
    "    'possible_is_conjugated_list': [False, True]\n",
    "}\n",
    "\n",
    "def safe_index(l, e):\n",
    "    try:\n",
    "        return l.index(e)\n",
    "    except:\n",
    "        return len(l) - 1\n",
    "\n",
    "def atom_to_feature_vector(atom):\n",
    "    atom_feature = [\n",
    "        safe_index(allowable_features['possible_atomic_num_list'], atom.GetAtomicNum()),\n",
    "        safe_index(allowable_features['possible_chirality_list'], str(atom.GetChiralTag())),\n",
    "        safe_index(allowable_features['possible_degree_list'], atom.GetTotalDegree()),\n",
    "        safe_index(allowable_features['possible_formal_charge_list'], atom.GetFormalCharge()),\n",
    "        safe_index(allowable_features['possible_numH_list'], atom.GetTotalNumHs()),\n",
    "        safe_index(allowable_features['possible_number_radical_e_list'], atom.GetNumRadicalElectrons()),\n",
    "        safe_index(allowable_features['possible_hybridization_list'], str(atom.GetHybridization())),\n",
    "        allowable_features['possible_is_aromatic_list'].index(atom.GetIsAromatic()),\n",
    "        allowable_features['possible_is_in_ring_list'].index(atom.IsInRing()),\n",
    "    ]\n",
    "    return atom_feature\n",
    "\n",
    "def bond_to_feature_vector(bond):\n",
    "    bond_feature = [\n",
    "        safe_index(allowable_features['possible_bond_type_list'], str(bond.GetBondType())),\n",
    "        allowable_features['possible_bond_stereo_list'].index(str(bond.GetStereo())),\n",
    "        allowable_features['possible_is_conjugated_list'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return bond_feature\n",
    "\n",
    "def smiles2graph(smiles_string):\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features_list.append(atom_to_feature_vector(atom))\n",
    "    x = np.array(atom_features_list, dtype=np.int64)\n",
    "    \n",
    "    num_bond_features = 3\n",
    "    if len(mol.GetBonds()) > 0:\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_feature = bond_to_feature_vector(bond)\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "        edge_index = np.array(edges_list, dtype=np.int64).T\n",
    "        edge_attr = np.array(edge_features_list, dtype=np.int64)\n",
    "    else:\n",
    "        edge_index = np.empty((2, 0), dtype=np.int64)\n",
    "        edge_attr = np.empty((0, num_bond_features), dtype=np.int64)\n",
    "    \n",
    "    graph = dict()\n",
    "    graph['edge_index'] = edge_index\n",
    "    graph['edge_feat'] = edge_attr\n",
    "    graph['node_feat'] = x\n",
    "    graph['num_nodes'] = len(x)\n",
    "    return graph\n",
    "# ===== END OGB IMPLEMENTATION ====='''\n",
    "        \n",
    "        # Patch files that use OGB\n",
    "        ogb_files = [\n",
    "            writable_grit / \"grit\" / \"loader\" / \"dataset\" / \"peptides_structural.py\",\n",
    "            writable_grit / \"grit\" / \"loader\" / \"dataset\" / \"peptides_functional.py\"\n",
    "        ]\n",
    "        \n",
    "        for ogb_file in ogb_files:\n",
    "            if ogb_file.exists():\n",
    "                with open(ogb_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                if \"from ogb.utils import smiles2graph\" in content:\n",
    "                    content = content.replace(\"from ogb.utils import smiles2graph\", ogb_implementation)\n",
    "                    with open(ogb_file, 'w') as f:\n",
    "                        f.write(content)\n",
    "                    print(f\"  ‚úÖ Patched: {ogb_file.name}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GRIT source not found (expected in local testing)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GRIT setup failed: {e}\")\n",
    "\n",
    "# Execute pipeline\n",
    "try:\n",
    "    spec = importlib.util.spec_from_file_location(\"kaggle_pipeline\", kaggle_pipeline)\n",
    "    pipeline_module = importlib.util.module_from_spec(spec)\n",
    "    \n",
    "    # Set command line args\n",
    "    original_argv = sys.argv.copy()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    sys.argv = ['full_pipeline_kaggle.py', '--cfg', str(dynamic_config_path), '--device', device]\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Config: {dynamic_config_path}\")\n",
    "    print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "    \n",
    "    print(f\"\\nüéõÔ∏è TRAINING WITH NOTEBOOK HYPERPARAMETERS:\")\n",
    "    print(f\"  üéØ Learning Rate: {BASE_LR}\")\n",
    "    print(f\"  üéØ GT Layers: {GT_LAYERS}\")\n",
    "    print(f\"  üéØ Max Epochs: {MAX_EPOCH}\")\n",
    "    \n",
    "    # Execute\n",
    "    spec.loader.exec_module(pipeline_module)\n",
    "    if hasattr(pipeline_module, 'main'):\n",
    "        pipeline_module.main()\n",
    "        print(f\"\\nüéâ Training completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üìã This may be expected in local testing - will work on Kaggle\")\n",
    "    \n",
    "finally:\n",
    "    sys.argv = original_argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Results Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"üìä TRAINING RESULTS ANALYSIS\")  \n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for results files using KAGGLE_WORKING paths\n",
    "submission_file = KAGGLE_WORKING / \"submission.csv\"\n",
    "eval_report = KAGGLE_WORKING / \"stage1_evaluation_report.csv\"\n",
    "results_dir = KAGGLE_WORKING / \"results\"\n",
    "\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"üìÑ Submission file: {submission_file}\")\n",
    "print(f\"üìã Evaluation report: {eval_report}\")\n",
    "\n",
    "# Analyze submission file\n",
    "if submission_file.exists():\n",
    "    print(f\"\\n‚úÖ Submission file found: {submission_file}\")\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "        submission = pd.read_csv(submission_file)\n",
    "        print(f\"üìä Submission shape: {submission.shape}\")\n",
    "        \n",
    "        # Show column info\n",
    "        expected_cols = ['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        actual_cols = list(submission.columns)\n",
    "        print(f\"üìã Columns: {actual_cols}\")\n",
    "        \n",
    "        missing_cols = set(expected_cols) - set(actual_cols)\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Show prediction statistics\n",
    "        print(f\"\\nüìà Prediction Statistics:\")\n",
    "        for target in ['Tg', 'FFV', 'Tc', 'Density', 'Rg']:\n",
    "            if target in submission.columns:\n",
    "                values = submission[target]\n",
    "                print(f\"  {target:8s}: mean={values.mean():7.3f}, std={values.std():7.3f}, range=[{values.min():6.3f}, {values.max():6.3f}]\")\n",
    "        \n",
    "        # Show sample predictions\n",
    "        print(f\"\\nüìÑ Sample Predictions:\")\n",
    "        print(submission.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading submission: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå No submission file found\")\n",
    "\n",
    "# Analyze evaluation report\n",
    "if eval_report.exists():\n",
    "    print(f\"\\n‚úÖ Evaluation report found: {eval_report}\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        eval_df = pd.read_csv(eval_report)\n",
    "        print(\"\\nüèÜ Model Performance Summary:\")\n",
    "        \n",
    "        # Show available columns\n",
    "        display_cols = ['Target', 'Performance_Grade', 'Performance_Level', 'Relative_Error', 'MAE', 'Test_MAE']\n",
    "        available_cols = [col for col in display_cols if col in eval_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            print(eval_df[available_cols].to_string(index=False))\n",
    "        else:\n",
    "            print(eval_df.to_string(index=False))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading evaluation report: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå No evaluation report found\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Successfully used notebook hyperparameters:\")\n",
    "print(f\"  üéØ Learning Rate: {BASE_LR}\")\n",
    "print(f\"  üéØ GT Layers: {GT_LAYERS}\")  \n",
    "print(f\"  üéØ Hidden Dimension: {GT_DIM_HIDDEN}\")\n",
    "print(f\"  üéØ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  üéØ Max Epochs: {MAX_EPOCH}\")\n",
    "print(f\"\\nüí° To tune performance:\")\n",
    "print(\"  1. Modify variables in Cell 2\")\n",
    "print(\"  2. Re-run cells 2-6\") \n",
    "print(\"  3. Compare results with previous runs\")\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"  üìÑ Submission: {submission_file}\")\n",
    "print(f\"  üìã Evaluation: {eval_report}\")\n",
    "print(f\"  üìÅ Results: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Hyperparameter Tuning Guide\n",
    "\n",
    "## üìã Quick Reference\n",
    "\n",
    "To adjust model performance, modify the variables in **Cell 2** (Hyperparameter Variables) and re-run the notebook.\n",
    "\n",
    "### üéØ High Priority Parameters (Major Impact)\n",
    "\n",
    "| Parameter | Current | Recommended Values | Impact |\n",
    "|-----------|---------|-------------------|---------|\n",
    "| `BASE_LR` | 1e-3 | [1e-4, 5e-4, 1e-3, 2e-3] | Learning speed & convergence |\n",
    "| `GT_LAYERS` | 10 | [8, 10, 12, 14] | Model capacity & training time |\n",
    "| `GT_DIM_HIDDEN` | 64 | [64, 128, 192] | Model width & memory usage |\n",
    "| `BATCH_SIZE` | 32 | [16, 32, 64] | Training stability & memory |\n",
    "\n",
    "### üéöÔ∏è Medium Priority Parameters\n",
    "\n",
    "| Parameter | Current | Recommended Values | Impact |\n",
    "|-----------|---------|-------------------|---------|\n",
    "| `GT_DROPOUT` | 0.0 | [0.0, 0.1, 0.2] | Regularization strength |\n",
    "| `GT_N_HEADS` | 8 | [4, 6, 8, 12] | Attention mechanism |\n",
    "| `WEIGHT_DECAY` | 1e-5 | [1e-6, 1e-5, 1e-4] | L2 regularization |\n",
    "| `MAX_EPOCH` | 200 | [150, 200, 300] | Training duration |\n",
    "\n",
    "## üöÄ Tuning Strategy\n",
    "\n",
    "### For Better Performance:\n",
    "- **Increase**: `GT_LAYERS` (10‚Üí12), `GT_DIM_HIDDEN` (64‚Üí128)\n",
    "- **Adjust**: `BASE_LR` (try 5e-4 or 2e-3)\n",
    "- **Add regularization**: `GT_DROPOUT` (0.0‚Üí0.1)\n",
    "\n",
    "### For Faster Training:\n",
    "- **Decrease**: `GT_LAYERS` (10‚Üí8), `MAX_EPOCH` (200‚Üí150)\n",
    "- **Increase**: `BATCH_SIZE` (32‚Üí64)\n",
    "\n",
    "### For Memory Issues:\n",
    "- **Decrease**: `BATCH_SIZE` (32‚Üí16), `GT_DIM_HIDDEN` (64‚Üí32)\n",
    "- **Reduce**: `GT_LAYERS` (10‚Üí8)\n",
    "\n",
    "## üìä Model Performance Grades\n",
    "\n",
    "The notebook will show performance grades for each target:\n",
    "- **A+/A**: Excellent (< 8% relative error)\n",
    "- **B+/B**: Good (< 18% relative error)  \n",
    "- **C**: Acceptable (< 25% relative error)\n",
    "- **D**: Poor (> 25% relative error)\n",
    "\n",
    "## üîÑ How to Tune\n",
    "\n",
    "1. **Modify variables** in Cell 2 (Hyperparameter Variables)\n",
    "2. **Re-run** the entire notebook from Cell 2 onwards\n",
    "3. **Check results** in the final cell for performance grades\n",
    "4. **Iterate** based on performance and time constraints\n",
    "\n",
    "## üí° Pro Tips\n",
    "\n",
    "- Start with learning rate: try `BASE_LR = 5e-4` or `2e-3`\n",
    "- For overfitting: increase `GT_DROPOUT` to 0.1-0.2\n",
    "- For underfitting: increase `GT_LAYERS` or `GT_DIM_HIDDEN`\n",
    "- Ensure `GT_DIM_HIDDEN` is divisible by `GT_N_HEADS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Results verification\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã RESULTS VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check submission file\n",
    "submission_path = KAGGLE_WORKING / 'submission.csv'\n",
    "if submission_path.exists():\n",
    "    print(f\"‚úÖ Submission file created: {submission_path}\")\n",
    "    \n",
    "    try:\n",
    "        import pandas as pd\n",
    "        submission = pd.read_csv(submission_path)\n",
    "        print(f\"üìä Submission shape: {submission.shape}\")\n",
    "        \n",
    "        # Check required columns\n",
    "        expected_cols = ['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        actual_cols = list(submission.columns)\n",
    "        \n",
    "        print(f\"üìã Columns found: {actual_cols}\")\n",
    "        missing_cols = set(expected_cols) - set(actual_cols)\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Show prediction statistics\n",
    "        print(\"\\nüìà Prediction Statistics:\")\n",
    "        for target in ['Tg', 'FFV', 'Tc', 'Density', 'Rg']:\n",
    "            if target in submission.columns:\n",
    "                values = submission[target]\n",
    "                print(f\"  {target:8s}: mean={values.mean():7.3f}, std={values.std():7.3f}, range=[{values.min():6.3f}, {values.max():6.3f}]\")\n",
    "        \n",
    "        # Show preview\n",
    "        print(f\"\\nüìÑ Submission Preview:\")\n",
    "        print(submission.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing submission: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå No submission file found at: {submission_path}\")\n",
    "\n",
    "# Check evaluation report\n",
    "eval_report_path = KAGGLE_WORKING / 'stage1_evaluation_report.csv'\n",
    "if eval_report_path.exists():\n",
    "    print(f\"\\n‚úÖ Training evaluation report: {eval_report_path}\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        eval_df = pd.read_csv(eval_report_path)\n",
    "        print(\"üèÜ Model Performance Summary:\")\n",
    "        display_cols = ['Target', 'Performance_Grade', 'Performance_Level', 'Relative_Error']\n",
    "        available_cols = [col for col in display_cols if col in eval_df.columns]\n",
    "        if available_cols:\n",
    "            print(eval_df[available_cols].to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading evaluation report: {e}\")\n",
    "\n",
    "# Final status\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ FINAL STATUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "status_checks = [\n",
    "    (\"Competition data found\", TRAIN_CSV.exists() and TEST_CSV.exists()),\n",
    "    (\"GRIT copied successfully\", (KAGGLE_WORKING / \"GRIT\").exists()),\n",
    "    (\"Configuration created\", (KAGGLE_WORKING / 'polymer-GRIT-RRWP.yaml').exists()),  # Fixed: use correct path\n",
    "    (\"Pipeline adapted\", (KAGGLE_WORKING / 'full_pipeline_kaggle.py').exists()),   # Fixed: use correct path\n",
    "    (\"Submission generated\", submission_path.exists()),\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for check_name, status in status_checks:\n",
    "    icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{icon} {check_name}\")\n",
    "    if not status:\n",
    "        all_good = False\n",
    "\n",
    "if all_good and submission_path.exists():\n",
    "    print(f\"\\nüéâ SUCCESS! Kaggle submission ready\")\n",
    "    print(f\"üìÑ Submit file: /kaggle/working/submission.csv\")\n",
    "    print(f\"üì¶ File size: {submission_path.stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Issues detected - check error messages above\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "isSourceIdPinned": false,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 8023087,
     "sourceId": 12695244,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 420493,
     "modelInstanceId": 402543,
     "sourceId": 507106,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
