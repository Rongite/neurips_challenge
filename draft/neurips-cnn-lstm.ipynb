{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f931e86",
   "metadata": {
    "papermill": {
     "duration": 0.004392,
     "end_time": "2025-07-03T02:07:49.321561",
     "exception": false,
     "start_time": "2025-07-03T02:07:49.317169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Important Note on Scoring and Execution\n",
    "\n",
    "    Without relying on the \"leaked\" data (i.e., test samples present in the training set), this notebook achieves a score of approximately 0.065.\n",
    "\n",
    "    The score may vary slightly between different runs. This is an expected behavior due to the data augmentation technique (random SMILES generation) used during both training and inference (TTA).\n",
    "\n",
    "    For faster execution, you can skip the lengthy training process and directly run the inference part. To do this, ensure the INFERENCE_MODELS_PATH variable points to the directory containing the pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fa9ce",
   "metadata": {
    "papermill": {
     "duration": 0.003022,
     "end_time": "2025-07-03T02:07:49.328135",
     "exception": false,
     "start_time": "2025-07-03T02:07:49.325113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6000bf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:49.336294Z",
     "iopub.status.busy": "2025-07-03T02:07:49.335558Z",
     "iopub.status.idle": "2025-07-03T02:07:56.307090Z",
     "shell.execute_reply": "2025-07-03T02:07:56.306513Z"
    },
    "papermill": {
     "duration": 6.977005,
     "end_time": "2025-07-03T02:07:56.308452",
     "exception": false,
     "start_time": "2025-07-03T02:07:49.331447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- RDKit ---\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit import RDLogger\n",
    "    # Disable RDKit logging to keep the output clean\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "except ImportError:\n",
    "    print(\"RDKit library is required. Please install it.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f8f2a1",
   "metadata": {
    "papermill": {
     "duration": 0.003148,
     "end_time": "2025-07-03T02:07:56.315096",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.311948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 2: Configuration and Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ef3de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:56.322289Z",
     "iopub.status.busy": "2025-07-03T02:07:56.321968Z",
     "iopub.status.idle": "2025-07-03T02:07:56.381014Z",
     "shell.execute_reply": "2025-07-03T02:07:56.380275Z"
    },
    "papermill": {
     "duration": 0.063986,
     "end_time": "2025-07-03T02:07:56.382192",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.318206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- General Settings ---\n",
    "SEED = 42\n",
    "TARGET_COLUMNS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODELS_OUTPUT_DIR = 'trained_models'\n",
    "\n",
    "# --- Data Paths ---\n",
    "# Note: Update these paths if your data is located elsewhere.\n",
    "# BASE_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "BASE_PATH = '/home/ubuntu/LLM-inference/jikai-project/neurips_challenge/data'\n",
    "EXTRA_DATA_BASE_PATH = '/home/ubuntu/LLM-inference/jikai-project/neurips_challenge/data/train_supplement'\n",
    "\n",
    "TRAIN_CSV_PATH = os.path.join(BASE_PATH, 'train.csv')\n",
    "TEST_CSV_PATH = os.path.join(BASE_PATH, 'test.csv')\n",
    "EXTRA_DATA_PATHS = {\n",
    "    'tc': os.path.join(EXTRA_DATA_BASE_PATH, 'dataset1.csv'),\n",
    "    'tg2': os.path.join(EXTRA_DATA_BASE_PATH, 'dataset2.csv'),\n",
    "    'tg3': os.path.join(EXTRA_DATA_BASE_PATH, 'dataset3.csv'),\n",
    "    'dnst': os.path.join(EXTRA_DATA_BASE_PATH, 'dataset4.csv')\n",
    "}\n",
    "# Path for loading models during inference\n",
    "INFERENCE_MODELS_PATH = 'trained_models/'\n",
    "\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "SMILES_EMBEDDING_DIM = 256\n",
    "\n",
    "# CNN branch\n",
    "CNN_FILTERS = [128, 128, 128, 128, 128, 128]\n",
    "CNN_KERNEL_SIZES = [5, 10, 15, 60, 70, 80]\n",
    "\n",
    "# LSTM branch\n",
    "LSTM_HIDDEN_SIZE = 128\n",
    "LSTM_NUM_LAYERS = 2\n",
    "\n",
    "# Fully Connected layers\n",
    "FC_LAYERS_SIZES = [512, 256]\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "DROPOUT_RATE = 0.4\n",
    "N_BINS_FOR_STRATIFY = 50 # For stratified splitting\n",
    "N_TTA = 5 # Number of Test-Time Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e711c1",
   "metadata": {
    "papermill": {
     "duration": 0.002931,
     "end_time": "2025-07-03T02:07:56.388486",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.385555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 3: Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21badae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:56.395511Z",
     "iopub.status.busy": "2025-07-03T02:07:56.395277Z",
     "iopub.status.idle": "2025-07-03T02:07:56.413949Z",
     "shell.execute_reply": "2025-07-03T02:07:56.413259Z"
    },
    "papermill": {
     "duration": 0.023547,
     "end_time": "2025-07-03T02:07:56.415030",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.391483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def make_smile_canonical(s):\n",
    "    \"\"\"Converts a SMILES string to its canonical form.\"\"\"\n",
    "    try:\n",
    "        return Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def add_extra_data(df_train, df_extra, target):\n",
    "    \"\"\"Merges external data into the main training dataframe.\"\"\"\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    \n",
    "    df_extra['SMILES'] = df_extra['SMILES'].apply(lambda s: make_smile_canonical(s))\n",
    "    df_extra = df_extra.dropna(subset=['SMILES', target])\n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    \n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "    \n",
    "    # Impute missing values in train_df with values from extra data\n",
    "    for smile in cross_smiles:\n",
    "        if pd.isnull(df_train.loc[df_train['SMILES']==smile, target]).any():\n",
    "            impute_value = df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "            df_train.loc[df_train['SMILES']==smile, target] = impute_value\n",
    "            \n",
    "    # Add new, unique SMILES from extra data\n",
    "    df_train = pd.concat([df_train, df_extra[df_extra['SMILES'].isin(unique_smiles_extra)]], axis=0, ignore_index=True)\n",
    "    \n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'For target \"{target}\": {n_samples_after - n_samples_before} new samples were added.')\n",
    "    return df_train\n",
    "\n",
    "# Set the seed for the entire notebook\n",
    "set_seed(SEED)\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd74071",
   "metadata": {
    "papermill": {
     "duration": 0.003028,
     "end_time": "2025-07-03T02:07:56.421267",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.418239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 4: Load and Integrate Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebec7592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:56.428586Z",
     "iopub.status.busy": "2025-07-03T02:07:56.428187Z",
     "iopub.status.idle": "2025-07-03T02:07:59.379380Z",
     "shell.execute_reply": "2025-07-03T02:07:59.378361Z"
    },
    "papermill": {
     "duration": 2.956194,
     "end_time": "2025-07-03T02:07:59.380714",
     "exception": false,
     "start_time": "2025-07-03T02:07:56.424520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process of loading and merging additional data...\n",
      "\n",
      "For target \"Tc\": 129 new samples were added.\n",
      "For target \"Tg\": 155 new samples were added.\n",
      "For target \"Tg\": 499 new samples were added.\n",
      "For target \"Density\": 634 new samples were added.\n",
      "\n",
      "--- Final number of samples for training ---\n",
      "\"Tg\": 1165\n",
      "\"FFV\": 7030\n",
      "\"Tc\": 866\n",
      "\"Density\": 1247\n",
      "\"Rg\": 614\n",
      "=============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Training file not found at {TRAIN_CSV_PATH}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Starting the process of loading and merging additional data...\\n\")\n",
    "\n",
    "try:\n",
    "    # Add Tc data\n",
    "    data_tc = pd.read_csv(EXTRA_DATA_PATHS['tc']).rename(columns={'TC_mean': 'Tc'})\n",
    "    train_df = add_extra_data(train_df, data_tc, 'Tc')\n",
    "\n",
    "    # Add Tg data (Source 2)\n",
    "    # Assuming dataset2 is the JCIM_sup_bigsmiles.csv and contains Tg data\n",
    "    # Since the original notebook uses 'Tg (C)', we will assume the column is named 'Tg'\n",
    "    data_tg2 = pd.read_csv(EXTRA_DATA_PATHS['tg2'])\n",
    "    if 'Tg (C)' in data_tg2.columns:\n",
    "        data_tg2 = data_tg2.rename(columns={'Tg (C)': 'Tg'})\n",
    "        train_df = add_extra_data(train_df, data_tg2, 'Tg')\n",
    "\n",
    "    # Add Tg data (Source 3)\n",
    "    data_tg3 = pd.read_csv(EXTRA_DATA_PATHS['tg3']).rename(columns={'Tg': 'Tg'})\n",
    "    # The original notebook converts from Kelvin to Celsius, we assume this is not needed for the provided csv\n",
    "    # data_tg3['Tg'] = data_tg3['Tg'] - 273.15\n",
    "    train_df = add_extra_data(train_df, data_tg3, 'Tg')\n",
    "\n",
    "    # Add Density data\n",
    "    # Assuming dataset4 is the density data\n",
    "    data_dnst = pd.read_csv(EXTRA_DATA_PATHS['dnst']).rename(columns={'FFV': 'Density'}) # Assuming FFV is density\n",
    "    data_dnst['SMILES'] = data_dnst['SMILES'].apply(lambda s: make_smile_canonical(s))\n",
    "    data_dnst = data_dnst[(data_dnst['SMILES'].notnull()) & (data_dnst['Density'].notnull())]\n",
    "    data_dnst['Density'] = data_dnst['Density'].astype('float64')\n",
    "    # The adjustment of -0.118 is kept as it is mentioned in the problem context\n",
    "    data_dnst['Density'] -= 0.118\n",
    "    train_df = add_extra_data(train_df, data_dnst, 'Density')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: Additional data file not found: {e.filename}\")\n",
    "    exit()\n",
    "\n",
    "print('\\n' + '--- Final number of samples for training ---')\n",
    "for t in TARGET_COLUMNS:\n",
    "    print(f'\\"{t}\\": {len(train_df[train_df[t].notnull()])}')\n",
    "print('='*45 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bcc2e",
   "metadata": {
    "papermill": {
     "duration": 0.003406,
     "end_time": "2025-07-03T02:07:59.388007",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.384601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 5: Tokenization and Vocabulary Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f4b707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:59.395795Z",
     "iopub.status.busy": "2025-07-03T02:07:59.395476Z",
     "iopub.status.idle": "2025-07-03T02:07:59.408704Z",
     "shell.execute_reply": "2025-07-03T02:07:59.408040Z"
    },
    "papermill": {
     "duration": 0.018394,
     "end_time": "2025-07-03T02:07:59.409829",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.391435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 48\n",
      "Max SMILES Length (with padding): 326\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary from all SMILES strings in the combined dataset\n",
    "unique_smiles_chars = sorted(list(set(''.join(train_df['SMILES'].astype(str))) - set(['&', '!'])))\n",
    "\n",
    "# Character to integer mapping\n",
    "smiles_char_dict = {char: i + 1 for i, char in enumerate(unique_smiles_chars)}\n",
    "smiles_char_dict['&'] = 0  # Padding token\n",
    "smiles_char_dict['!'] = len(smiles_char_dict) # Unknown token\n",
    "\n",
    "vocab_size = len(smiles_char_dict)\n",
    "\n",
    "# Determine max length for padding, adding some buffer\n",
    "max_smiles_len = train_df['SMILES'].astype(str).str.len().max() + 20\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Max SMILES Length (with padding): {max_smiles_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0d87a",
   "metadata": {
    "papermill": {
     "duration": 0.003354,
     "end_time": "2025-07-03T02:07:59.416752",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.413398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 6: PyTorch Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e701f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:59.424617Z",
     "iopub.status.busy": "2025-07-03T02:07:59.424068Z",
     "iopub.status.idle": "2025-07-03T02:07:59.429850Z",
     "shell.execute_reply": "2025-07-03T02:07:59.429297Z"
    },
    "papermill": {
     "duration": 0.010679,
     "end_time": "2025-07-03T02:07:59.430831",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.420152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, smiles_list, targets, char_dict, max_len, is_train=False):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.targets = targets\n",
    "        self.char_dict = char_dict\n",
    "        self.max_len = max_len\n",
    "        self.pad_token_id = char_dict['&']\n",
    "        self.unknown_token_id = char_dict['!']\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        # SMILES augmentation for training\n",
    "        if self.is_train:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "        \n",
    "        # Tokenize and pad\n",
    "        tokenized_smiles = [self.char_dict.get(char, self.unknown_token_id) for char in smiles]\n",
    "        padding_len = self.max_len - len(tokenized_smiles)\n",
    "        padded_smiles = tokenized_smiles + [self.pad_token_id] * padding_len\n",
    "        \n",
    "        return (torch.tensor(padded_smiles[:self.max_len], dtype=torch.long),\n",
    "                torch.tensor(target, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859ad33",
   "metadata": {
    "papermill": {
     "duration": 0.003159,
     "end_time": "2025-07-03T02:07:59.437432",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.434273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 7: Model Definition (CNN-LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604a4d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:59.445416Z",
     "iopub.status.busy": "2025-07-03T02:07:59.445169Z",
     "iopub.status.idle": "2025-07-03T02:07:59.453332Z",
     "shell.execute_reply": "2025-07-03T02:07:59.452769Z"
    },
    "papermill": {
     "duration": 0.013076,
     "end_time": "2025-07-03T02:07:59.454267",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.441191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolymerPredictor(nn.Module):\n",
    "    def __init__(self, smiles_vocab_size, smiles_embedding_dim, cnn_filters, cnn_kernel_sizes,\n",
    "                 lstm_hidden_size, lstm_num_layers, fc_layers_sizes, dropout_rate=0.25):\n",
    "        super(PolymerPredictor, self).__init__()\n",
    "\n",
    "        self.smiles_embedding = nn.Embedding(smiles_vocab_size, smiles_embedding_dim, padding_idx=smiles_char_dict['&'])\n",
    "        \n",
    "        # --- CNN Branch ---\n",
    "        self.parallel_cnns = nn.ModuleList([\n",
    "            nn.Conv1d(smiles_embedding_dim, out_ch, ks) for out_ch, ks in zip(cnn_filters, cnn_kernel_sizes)\n",
    "        ])\n",
    "        self.smi_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # --- LSTM Branch ---\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=smiles_embedding_dim,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # --- Fully Connected (FC) Layers ---\n",
    "        fc_input_size = sum(cnn_filters) + (lstm_hidden_size * 2) # *2 for bidirectional LSTM\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for layer_size in fc_layers_sizes:\n",
    "            self.fc_layers.append(nn.Linear(fc_input_size, layer_size))\n",
    "            fc_input_size = layer_size\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(fc_layers_sizes[-1], 1)\n",
    "\n",
    "    def forward(self, smiles_tensor, return_activations=False):\n",
    "        # --- Input Processing ---\n",
    "        # Input for CNN: (batch, embed_dim, seq_len)\n",
    "        smi_x_embedded_cnn = self.smiles_embedding(smiles_tensor).permute(0, 2, 1)\n",
    "        # Input for LSTM: (batch, seq_len, embed_dim)\n",
    "        smi_x_embedded_lstm = smi_x_embedded_cnn.permute(0, 2, 1)\n",
    "\n",
    "        # --- CNN Branch Processing ---\n",
    "        cnn_outputs = [self.smi_pool(torch.relu(cnn(smi_x_embedded_cnn))).squeeze(2) for cnn in self.parallel_cnns]\n",
    "        cnn_features = torch.cat(cnn_outputs, dim=1)\n",
    "        \n",
    "        # --- LSTM Branch Processing ---\n",
    "        _, (h_n, _) = self.lstm(smi_x_embedded_lstm)\n",
    "        # Concatenate the final hidden states from forward and backward directions\n",
    "        lstm_features = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "\n",
    "        # --- Combine Branches and Final Layers ---\n",
    "        combined_features = torch.cat((cnn_features, lstm_features), dim=1)\n",
    "        \n",
    "        x = self.dropout(combined_features)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = torch.relu(fc_layer(x))\n",
    "        \n",
    "        final_prediction = self.output_layer(x).squeeze(1)\n",
    "\n",
    "        if return_activations:\n",
    "            return final_prediction, cnn_features, lstm_features\n",
    "        else:\n",
    "            return final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159a11c",
   "metadata": {
    "papermill": {
     "duration": 0.00322,
     "end_time": "2025-07-03T02:07:59.460879",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.457659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 8: Training and Evaluation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300d5068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:59.468458Z",
     "iopub.status.busy": "2025-07-03T02:07:59.468223Z",
     "iopub.status.idle": "2025-07-03T02:07:59.477089Z",
     "shell.execute_reply": "2025-07-03T02:07:59.476568Z"
    },
    "papermill": {
     "duration": 0.013766,
     "end_time": "2025-07-03T02:07:59.477968",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.464202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs, device):\n",
    "    \"\"\"The main training and validation loop.\"\"\"\n",
    "    model.to(device)\n",
    "    best_val_loss = np.inf\n",
    "    best_model_path = 'best_model_temp.pth'\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        \n",
    "        for smiles, targets in progress_bar:\n",
    "            smiles, targets = smiles.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(smiles)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for smiles, targets in val_loader:\n",
    "                smiles, targets = smiles.to(device), targets.to(device)\n",
    "                predictions = model(smiles)\n",
    "                loss = criterion(predictions, targets)\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> Train Loss: {avg_train_loss:.8f}, Val Loss: {avg_val_loss:.8f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"  -> Better model found! Val Loss: {best_val_loss:.8f}. Model saved.\")\n",
    "            \n",
    "    print(f\"\\nTraining finished. Loading best model with Val Loss: {best_val_loss:.8f}\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    if os.path.exists(best_model_path):\n",
    "        os.remove(best_model_path) # Clean up temporary file\n",
    "        \n",
    "    return model\n",
    "\n",
    "def analyze_branch_importance(model, val_loader, device):\n",
    "    \"\"\"Analyzes the average activation magnitude from CNN and LSTM branches.\"\"\"\n",
    "    model.eval()\n",
    "    cnn_total_magnitude = 0.0\n",
    "    lstm_total_magnitude = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for smiles, _ in tqdm(val_loader, desc=\"Analyzing Branch Importance\", leave=False):\n",
    "            smiles = smiles.to(device)\n",
    "            _, cnn_activations, lstm_activations = model(smiles, return_activations=True)\n",
    "            \n",
    "            cnn_total_magnitude += torch.mean(torch.abs(cnn_activations)).item()\n",
    "            lstm_total_magnitude += torch.mean(torch.abs(lstm_activations)).item()\n",
    "            num_batches += 1\n",
    "            \n",
    "    cnn_avg_mag = cnn_total_magnitude / num_batches\n",
    "    lstm_avg_mag = lstm_total_magnitude / num_batches\n",
    "    \n",
    "    print(\"\\n--- Parallel Branch Importance Analysis (CNN vs. LSTM) ---\")\n",
    "    print(f\"Average Activation Magnitude of CNN Branch : {cnn_avg_mag:.6f}\")\n",
    "    print(f\"Average Activation Magnitude of LSTM Branch: {lstm_avg_mag:.6f}\")\n",
    "    if cnn_avg_mag > lstm_avg_mag:\n",
    "        print(\">> Conclusion: The CNN branch sends a stronger signal to the final layers.\")\n",
    "    else:\n",
    "        print(\">> Conclusion: The LSTM branch sends a stronger signal to the final layers.\")\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed06fe0",
   "metadata": {
    "papermill": {
     "duration": 0.003162,
     "end_time": "2025-07-03T02:07:59.484487",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.481325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 9: The Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f07dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T02:07:59.491990Z",
     "iopub.status.busy": "2025-07-03T02:07:59.491814Z",
     "iopub.status.idle": "2025-07-03T04:06:50.409067Z",
     "shell.execute_reply": "2025-07-03T04:06:50.408246Z"
    },
    "papermill": {
     "duration": 7131.470949,
     "end_time": "2025-07-03T04:06:50.958779",
     "exception": false,
     "start_time": "2025-07-03T02:07:59.487830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(MODELS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for target_col in TARGET_COLUMNS:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Starting process for target column: {target_col}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df_filtered = train_df.dropna(subset=[target_col]).copy().reset_index(drop=True)\n",
    "    \n",
    "    if len(df_filtered) < 20:\n",
    "        print(f\"Not enough data for '{target_col}'. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Attempt stratified split to maintain target distribution\n",
    "    try:\n",
    "        y_bins = pd.qcut(df_filtered[target_col], q=N_BINS_FOR_STRATIFY, labels=False, duplicates='drop')\n",
    "        stratify_param = y_bins\n",
    "        print(f\"Performing stratified split for '{target_col}'.\")\n",
    "    except ValueError:\n",
    "        stratify_param = None\n",
    "        print(f\"Performing normal split for '{target_col}'.\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df_filtered['SMILES'].values,\n",
    "        df_filtered[target_col].values,\n",
    "        test_size=0.25,\n",
    "        random_state=SEED,\n",
    "        stratify=stratify_param\n",
    "    )\n",
    "    \n",
    "    train_dataset = PolymerDataset(X_train, y_train, smiles_char_dict, max_smiles_len, is_train=True)\n",
    "    val_dataset = PolymerDataset(X_val, y_val, smiles_char_dict, max_smiles_len, is_train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = PolymerPredictor(\n",
    "        smiles_vocab_size=vocab_size,\n",
    "        smiles_embedding_dim=SMILES_EMBEDDING_DIM,\n",
    "        cnn_filters=CNN_FILTERS,\n",
    "        cnn_kernel_sizes=CNN_KERNEL_SIZES,\n",
    "        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n",
    "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
    "        fc_layers_sizes=FC_LAYERS_SIZES,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.L1Loss() # Mean Absolute Error\n",
    "    \n",
    "    print(\"Starting training of the hybrid CNN-LSTM model...\")\n",
    "    trained_model = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, EPOCHS, DEVICE)\n",
    "    \n",
    "    # Analyze branch importance after training\n",
    "    analyze_branch_importance(trained_model, val_loader, DEVICE)\n",
    "    \n",
    "    model_save_path = os.path.join(MODELS_OUTPUT_DIR, f'model_cnn_lstm_{target_col}.pth')\n",
    "    torch.save(trained_model.state_dict(), model_save_path)\n",
    "    print(f\"Model for {target_col} successfully saved to '{model_save_path}'.\")\n",
    "\n",
    "print(\"\\nTraining process finished for all target columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c84d84",
   "metadata": {
    "papermill": {
     "duration": 6.740843,
     "end_time": "2025-07-03T04:07:04.510640",
     "exception": false,
     "start_time": "2025-07-03T04:06:57.769797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 10: Helper Function for Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09438f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:07:17.667728Z",
     "iopub.status.busy": "2025-07-03T04:07:17.666954Z",
     "iopub.status.idle": "2025-07-03T04:07:17.671801Z",
     "shell.execute_reply": "2025-07-03T04:07:17.671064Z"
    },
    "papermill": {
     "duration": 6.605193,
     "end_time": "2025-07-03T04:07:17.672960",
     "exception": false,
     "start_time": "2025-07-03T04:07:11.067767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smiles_to_tensor(smiles, char_dict, max_len):\n",
    "    \"\"\"Converts a single SMILES string to a padded tensor.\"\"\"\n",
    "    pad_token_id = char_dict['&']\n",
    "    unknown_token_id = char_dict['!']\n",
    "    tokenized = [char_dict.get(c, unknown_token_id) for c in smiles]\n",
    "    tokenized = tokenized[:max_len]\n",
    "    padding = [pad_token_id] * (max_len - len(tokenized))\n",
    "    return torch.tensor(tokenized + padding, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad49912",
   "metadata": {
    "papermill": {
     "duration": 6.725802,
     "end_time": "2025-07-03T04:07:30.991515",
     "exception": false,
     "start_time": "2025-07-03T04:07:24.265713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Cell 11: Generate Predictions for Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04ac8c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T04:07:44.209324Z",
     "iopub.status.busy": "2025-07-03T04:07:44.208637Z",
     "iopub.status.idle": "2025-07-03T04:07:46.050299Z",
     "shell.execute_reply": "2025-07-03T04:07:46.049358Z"
    },
    "papermill": {
     "duration": 8.406019,
     "end_time": "2025-07-03T04:07:46.051587",
     "exception": false,
     "start_time": "2025-07-03T04:07:37.645568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test data...\n",
      "Test data loaded successfully.\n",
      "\n",
      "Predicting for column: Tg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Tg: 100%|██████████| 3/3 [00:00<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for column: FFV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting FFV: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for column: Tc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Tc: 100%|██████████| 3/3 [00:00<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for column: Density\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Density: 100%|██████████| 3/3 [00:00<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for column: Rg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting Rg: 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "submission.csv file created successfully.\n",
      "Sample of the output:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  167.816630  0.375729  0.215749  1.132733  23.194366\n",
      "1  1422188626  176.811480  0.378414  0.251642  1.056754  21.202372\n",
      "2  2032016830  141.433996  0.351483  0.265111  1.117257  18.583658\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading test data...\")\n",
    "try:\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    test_df['SMILES'] = test_df['SMILES'].apply(lambda s: make_smile_canonical(s))\n",
    "    print(\"Test data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Test file not found at {TEST_CSV_PATH}\")\n",
    "    exit()\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_df['id']})\n",
    "\n",
    "for target_col in TARGET_COLUMNS:\n",
    "    print(f\"\\nPredicting for column: {target_col}\")\n",
    "    model_path = os.path.join(INFERENCE_MODELS_PATH, f'model_cnn_lstm_{target_col}.pth')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Warning: Model not found at '{model_path}'. Using mean value as fallback.\")\n",
    "        # Fallback to mean of the original training data if a model is missing\n",
    "        original_train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "        submission_df[target_col] = original_train_df[target_col].mean()\n",
    "        continue\n",
    "\n",
    "    model = PolymerPredictor(\n",
    "        smiles_vocab_size=vocab_size,\n",
    "        smiles_embedding_dim=SMILES_EMBEDDING_DIM,\n",
    "        cnn_filters=CNN_FILTERS,\n",
    "        cnn_kernel_sizes=CNN_KERNEL_SIZES,\n",
    "        lstm_hidden_size=LSTM_HIDDEN_SIZE,\n",
    "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
    "        fc_layers_sizes=FC_LAYERS_SIZES,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    final_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"Predicting {target_col}\"):\n",
    "            original_smiles = row['SMILES']\n",
    "            mol = Chem.MolFromSmiles(original_smiles)\n",
    "            tta_predictions = []\n",
    "            \n",
    "            # Prediction on original canonical SMILES\n",
    "            original_tensor = smiles_to_tensor(original_smiles, smiles_char_dict, max_smiles_len).unsqueeze(0).to(DEVICE)\n",
    "            pred_original = model(original_tensor).item()\n",
    "            tta_predictions.append(pred_original)\n",
    "            \n",
    "            # Test-Time Augmentation (TTA)\n",
    "            if mol is not None:\n",
    "                for _ in range(N_TTA):\n",
    "                    aug_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "                    aug_tensor = smiles_to_tensor(aug_smiles, smiles_char_dict, max_smiles_len).unsqueeze(0).to(DEVICE)\n",
    "                    pred_aug = model(aug_tensor).item()\n",
    "                    tta_predictions.append(pred_aug)\n",
    "            \n",
    "            final_pred = np.mean(tta_predictions)\n",
    "            final_predictions.append(final_pred)\n",
    "            \n",
    "    submission_df[target_col] = final_predictions\n",
    "    \n",
    "    # --- Handle Data Leak ---\n",
    "    # Replace predictions with known values for test SMILES that were in the training set\n",
    "    leak_df = train_df.dropna(subset=[target_col])\n",
    "    leak_dict = pd.Series(leak_df[target_col].values, index=leak_df.SMILES).to_dict()\n",
    "    leaked_values = test_df['SMILES'].map(leak_dict)\n",
    "    \n",
    "    submission_df[target_col] = np.where(leaked_values.notna(), leaked_values, submission_df[target_col])\n",
    "\n",
    "# --- Save Final Submission File ---\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"submission.csv file created successfully.\")\n",
    "print(\"Sample of the output:\")\n",
    "print(submission_df.head())\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12609125,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7709869,
     "sourceId": 12330396,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7210.330448,
   "end_time": "2025-07-03T04:07:55.573537",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-03T02:07:45.243089",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
